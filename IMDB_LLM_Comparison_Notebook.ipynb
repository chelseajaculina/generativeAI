{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "a3d2abad", "cell_type": "markdown", "source": "## \ud83e\udde9 Step 0: Setup Environment", "metadata": {}}, {"id": "ab8dbbc7", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install transformers datasets accelerate fsspec==2023.6.0 --quiet\nfrom huggingface_hub import login\n\n# Optional: Paste your token here\nHUGGINGFACE_TOKEN = \"hf_your_token_here\"  # Replace with your token\nlogin(token=HUGGINGFACE_TOKEN)\n", "outputs": []}, {"id": "5c1c23b0", "cell_type": "markdown", "source": "## \ud83d\udce6 Step 1: Load & Sample 200 IMDB Reviews", "metadata": {}}, {"id": "9a1a4bfa", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from datasets import load_dataset\nimport random\nimport pandas as pd\n\nimdb = load_dataset(\"imdb\")\nsample_data = random.sample(list(imdb['test']), 200)\ndf_sample = pd.DataFrame(sample_data)\ndf_sample.head()\n", "outputs": []}, {"id": "0765dcac", "cell_type": "markdown", "source": "## \ud83d\udd24 Step 2: Define Prompting Techniques", "metadata": {}}, {"id": "7a9a2f52", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "few_shot_examples = [\n    {\"text\": \"This movie was fantastic! The acting was top-notch and the story was gripping.\", \"label\": \"Positive\"},\n    {\"text\": \"I hated every minute of this film. It was boring and predictable.\", \"label\": \"Negative\"},\n    {\"text\": \"Absolutely loved the cinematography and emotional depth. Highly recommend!\", \"label\": \"Positive\"},\n    {\"text\": \"Terrible script and poor character development. Not worth the time.\", \"label\": \"Negative\"},\n    {\"text\": \"One of the best films I\u2019ve seen this year. Beautiful storytelling.\", \"label\": \"Positive\"},\n    {\"text\": \"The plot made no sense and the pacing was awful.\", \"label\": \"Negative\"},\n    {\"text\": \"Brilliant direction and a powerful message. A must-watch.\", \"label\": \"Positive\"},\n    {\"text\": \"The dialogue was cheesy and the acting felt forced.\", \"label\": \"Negative\"},\n    {\"text\": \"Incredible performance by the lead actor. It kept me hooked.\", \"label\": \"Positive\"},\n    {\"text\": \"This movie was a total disaster. Save yourself the trouble.\", \"label\": \"Negative\"}\n]\n\ndef zero_shot_prompt(text):\n    return f\"Classify the sentiment as Positive or Negative:\\n\\n{text}\\n\\nSentiment:\"\n\ndef few_shot_prompt(text):\n    prompt = \"Classify the sentiment of each review below:\\n\"\n    for ex in few_shot_examples:\n        prompt += f\"Review: {ex['text']}\\nSentiment: {ex['label']}\\n\\n\"\n    prompt += f\"Review: {text}\\nSentiment:\"\n    return prompt\n\ndef cot_prompt(text):\n    return f\"Review: {text}\\nExplain the sentiment (positive or negative) and then give your answer.\"\n\ndef self_consistency_prompt(text):\n    return cot_prompt(text)\n", "outputs": []}, {"id": "5b650dc0", "cell_type": "markdown", "source": "## \ud83e\udde0 Step 3: Load an LLM (DeepSeek / LLaVA / Mistral)", "metadata": {}}, {"id": "cf16c5a4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\ndef load_model(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    return pipe\n", "outputs": []}, {"id": "b9793232", "cell_type": "markdown", "source": "## \ud83e\uddea Step 4: Predict with Prompting Techniques", "metadata": {}}, {"id": "e19532cf", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def get_sentiment_from_output(output_text):\n    output_text = output_text.lower()\n    if \"positive\" in output_text:\n        return 1\n    elif \"negative\" in output_text:\n        return 0\n    return -1\n\ndef predict_sentiment(pipe, text, prompt_fn):\n    prompt = prompt_fn(text)\n    output = pipe(prompt, max_new_tokens=50, do_sample=False)[0][\"generated_text\"]\n    return get_sentiment_from_output(output)\n", "outputs": []}, {"id": "b90786e8", "cell_type": "markdown", "source": "## \ud83d\udd01 Step 5: Predict with Self-Consistency", "metadata": {}}, {"id": "542124eb", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "def predict_with_self_consistency(pipe, text, prompt_fn, samples=5):\n    preds = []\n    for _ in range(samples):\n        prompt = prompt_fn(text)\n        output = pipe(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n        pred = get_sentiment_from_output(output)\n        if pred != -1:\n            preds.append(pred)\n    if preds:\n        return max(set(preds), key=preds.count)\n    return -1\n", "outputs": []}, {"id": "259af68d", "cell_type": "markdown", "source": "## \ud83d\udcca Step 6: Evaluate Model Performance", "metadata": {}}, {"id": "de443681", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate(pipe, prompt_fn, use_self_consistency=False):\n    y_true, y_pred = [], []\n    for _, row in df_sample.iterrows():\n        true = row['label']\n        text = row['text']\n        pred = (\n            predict_with_self_consistency(pipe, text, prompt_fn)\n            if use_self_consistency else\n            predict_sentiment(pipe, text, prompt_fn)\n        )\n        if pred != -1:\n            y_true.append(true)\n            y_pred.append(pred)\n\n    return {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred),\n        \"recall\": recall_score(y_true, y_pred),\n        \"f1_score\": f1_score(y_true, y_pred)\n    }\n", "outputs": []}, {"id": "dbebc9cc", "cell_type": "markdown", "source": "## \ud83d\ude80 Step 7: Run All Combinations", "metadata": {}}, {"id": "6f556160", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "models = {\n    \"DeepSeek\": \"deepseek-ai/deepseek-llm-7b-chat\",\n    \"LLaVA\": \"liuhaotian/llava-v1.5-7b\",\n    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n}\n\nprompt_styles = {\n    \"zero_shot\": zero_shot_prompt,\n    \"few_shot\": few_shot_prompt,\n    \"chain_of_thought\": cot_prompt,\n    \"self_consistency\": self_consistency_prompt\n}\n\nresults = []\n\nfor model_name, model_id in models.items():\n    pipe = load_model(model_id)\n    for prompt_name, prompt_fn in prompt_styles.items():\n        metrics = evaluate(pipe, prompt_fn, use_self_consistency=(prompt_name == \"self_consistency\"))\n        results.append({\n            \"model\": model_name,\n            \"prompting\": prompt_name,\n            **metrics\n        })\n        print(f\"\u2705 Done: {model_name} + {prompt_name}\")\n", "outputs": []}, {"id": "03da2f27", "cell_type": "markdown", "source": "## \ud83d\udcc8 Step 8: Results Table", "metadata": {}}, {"id": "89513f03", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "results_df = pd.DataFrame(results)\nresults_df.sort_values(by=\"f1_score\", ascending=False)\n", "outputs": []}]}